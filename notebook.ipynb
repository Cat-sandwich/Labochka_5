{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Lemmatize(src: list):\n",
    "    ''' Лемматизирует переданный датасет\n",
    "    '''\n",
    "    text_nomalized = ' '.join(src).lower() \n",
    "\n",
    "    m = Mystem()\n",
    "    lemmas = m.lemmatize(text_nomalized)\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mark                                    text_of_comment\n",
      "0     1  Сразу отвечаю на вопрос, почему я решила прочи...\n",
      "1     1  Эта сказка хоть и сказка,но учит всех людей то...\n",
      "2     1  Несомненно видно мастерство автора, который с ...\n",
      "3     1  Эта сказка хоть и сказка,но учит всех людей то...\n",
      "4     1  Несомненно видно мастерство автора, который с ...\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('dataframe.csv')\n",
    "data = data.head(5)\n",
    "print(data)\n",
    "#data = data.head(10)\n",
    "#«Обучающая выборка» — это набор данных, который используется для разработки модели машинного обучения.\n",
    "\n",
    "#«Валидационная выборка» — это набор данных, который используется в процессе разработки модели машинного обучения для подбора оптимального набора гиперпараметров.\n",
    "\n",
    "#«Тестовая выборка» — это набор данных, который не используется непосредственно в процессе обучения модели или для подбора гиперпараметров, \n",
    "# однако позволяет протестировать модель и является контрольным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_non_alphabets\n",
      "tokenize\n",
      "stem\n",
      "Lemmatize\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "complete\n",
      "   mark                                    text_of_comment\n",
      "0     1  сразу отвечаю на вопрос почему я решила прочит...\n",
      "1     1  эта сказка хоть и сказка но учит всех людей то...\n",
      "2     1  несомненно видно мастерство автора который с п...\n",
      "3     1  эта сказка хоть и сказка но учит всех людей то...\n",
      "4     1  несомненно видно мастерство автора который с п...\n"
     ]
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "remove_non_alphabets = lambda x: re.sub(r'[^а-яА-Я]',' ',str(x))\n",
    "\n",
    "tokenize = lambda x: word_tokenize(x, language = \"russian\")\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stem = lambda w: [ ps.stem(x) for x in w ]\n",
    "\n",
    "print('remove_non_alphabets')\n",
    "data['text_of_comment'] = data['text_of_comment'].apply(remove_non_alphabets)\n",
    "\n",
    "print('tokenize')\n",
    "data['text_of_comment'] = data['text_of_comment'].apply(tokenize) # [ word_tokenize(row) for row in data['email']]\n",
    "\n",
    "print('stem')\n",
    "data['text_of_comment'] = data['text_of_comment'].apply(stem)\n",
    "\n",
    "print('Lemmatize')\n",
    "c = 0\n",
    "for words_list in data['text_of_comment']:\n",
    "    words_list = Lemmatize(words_list)\n",
    "    print(c)\n",
    "    c+=1\n",
    "\n",
    "print('complete')\n",
    "data['text_of_comment'] = data['text_of_comment'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 1 1 0 0 0 0 1 2 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 2 1 0 1 0\n",
      "  0 0 1 0 0 0 0 1 4 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 2 0 0 1 0 0\n",
      "  0 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 2 1 1 0 1 0 1 0 0 0\n",
      "  0 0 0 0 3 1 1 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 2]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0]\n",
      " [1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 2 1 0 1 1 0 1 1 0 0 2 0 1\n",
      "  1 1 0 0 1 1 2 0 0 1 1 0 2 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1\n",
      "  1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1\n",
      "  0 1 1 2 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 2 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0]\n",
      " [1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 2 1 0 1 1 0 1 1 0 0 2 0 1\n",
      "  1 1 0 0 1 1 2 0 0 1 1 0 2 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1\n",
      "  1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1\n",
      "  0 1 1 2 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 2 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 143)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_words = 10000\n",
    "stopWords = stopwords.words('russian')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopWords)\n",
    "sparse_matrix = vectorizer.fit_transform(data['text_of_comment']).toarray()\n",
    "print(sparse_matrix)\n",
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(sparse_matrix, np.array(data['mark']), test_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(10000, 100)\n",
    "        self.linear2 = nn.Linear(100, 10)\n",
    "        self.linear3 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
